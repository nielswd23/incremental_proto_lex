10/27/25 
Working on a different version of the incremental analysis. Original implementation was do the splits in corpus lines, segment some of the lines which were added to the wordlist, and then treat the rest of the unsegmented lines as words. Version 2 is to just throw out the unsegmented lines which might be overwhelming the phonotactic statistics and incrementally learn from less and less data. Surprisingly, this version 2 produces very different results. Previosuly, the model would fail to distinguish the contrasts after just 1/8 of the lines being unsegmented. In version 2 where unseg lines are thrown out, even as much as 1/2 of the lines being thrown out, the model still seems to be making the right contrasts. Thus, I need to edit how I am splitting the corpora in splitting_corpora.py.

We previously enforced disjoint samples when splitting. For example, if the split was 1/8, we could create 8 folds of the corpus, and each sample would keep 1/8 of the lines unsegmented where the lines that were unsegmented did not overlap across the samples. So for a 1/2 split, we could only create 2 samples. But if we want to increase beyond 1/2 of the corpus being unsegmented with a split factor of 1/1.5, we can no longer enforce unique unsegmented lines across samples. So, I relaxed this disjoint across samples restriction and extended the split factors. Because of this relaxation, I will also include 8 samples for each of the levels (previously, 1/2 could only have 2 samples and 1/4 could only have 4) for more consistent results.

10/15/25
Too many NAs when running the BayesianAnalysis with the ngram predictors for TinyInfantLexicon and top_22 because there were a lot of unseen sounds. Filtering out the words with unseen sounds in the analysis by removing any rows where uni_prob == -inf. 
Should also run the positional score analysis on these updated csvs because even though the positional score returns a usable number with unseen characters, we shouldn't be predicting this way.

10/15/25
Another error, but this seems more random and usually resolves when running again. Any harm in expanding the size?

The total size of the 17 globals exported for future expression (‘FUN()’) is 503.85 MiB. This exceeds the maximum allowed size 500.00 MiB per by R option "future.globals.maxSize". This limit is set to protect against transfering too large objects to parallel workers by mistake, which may not be intended and could be costly. See help("future.globals.maxSize", package = "future") for further explainations and how to adjust or remove this threshold The three largest globals are ‘FUN’ (172.38 MiB of class ‘function’), ‘up_args’ (166.44 MiB of class ‘list’) and ‘newdata’ (165.02 MiB of class ‘list’)
Calls: fit_and_predict_bayesian ... getGlobalsAndPackagesXApply -> getGlobalsAndPackages

10/13/25
Found that the error was from the t_a df when using median_hdi(). This creates a high density interval but if there are two  high density intervals, then it will output those two instead of the the one output interval. Here's how I did the troubleshooting: https://chatgpt.com/share/68edb3ac-b7c0-8001-8aa0-d6e06802e206 

10/8/25 
Whenever the bayesian analysis runs fail, it's always on the both contrast stimuli. The runs are outputting KFoldModel... for unigram but not both. Once I have run the rest of the models, I will add some print statements to the BayesianAnalysis script to see what the successful ones are printing and why some are failing. 

10/8/25 
TODO: edit format_for_ngramcalc.py to merge the "x" and "^" symbols to create formatted_corpora_merged
-- or maybe call the old one formatted_corpora_unmerged seeing that we'll probably stick with this analysis. Will also need to change incremental_corpora_out  
Then rerun the whole process. 
run_ngramcalc.py to get ScoredLists 
run BayesianAnalysis.R
Make figures 

10/2/25
A couple of errors that I'm encountering during runs of the BayesianAnalysis.
1. When running the PhilipsPearl_DMCMC_Bigram model:
[1] "kfolding"
Error in getGlobalsAndPackages(expr, envir = envir, globals = globals) :
  The total size of the 17 globals exported for future expression (‘FUN()’) is 503.87 MiB. This exceeds the maximum allowed size 500.00 MiB per by R option "future.globals.maxSize". This limit is set to protect against transfering too large objects to parallel workers by mistake, which may not be intended and could be costly. See help("future.globals.maxSize", package = "future") for further explainations and how to adjust or remove this threshold The three largest globals are ‘FUN’ (172.39 MiB of class ‘function’), ‘up_args’ (166.45 MiB of class ‘list’) and ‘newdata’ (165.02 MiB of class ‘list’)
Calls: fit_and_predict_bayesian ... getGlobalsAndPackagesXApply -> getGlobalsAndPackages
In addition: There were 44 warnings (use warnings() to see them)
Execution halted

2. Ran into an error when running the AGSimple model: 
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA group_by: one grouping variable (DrawNum) summarise: now 4,000 rows and 2 columns, ungrouped select: dropped one variable (DrawNum) mutate: new variable 'is_above_chance' (double) with one unique value and 0% NA new variable 'p_above_chance' (double) with one unique value and 0% NA new variable 'p_above_random' (double) with one unique value and 0% NA select: dropped 2 variables (accuracy, is_above_chance) distinct: removed 3,999 rows (>99%), one row remaining Error in data.frame(..., check.names = FALSE) : arguments imply differing number of rows: 2, 1 Calls: fit_and_predict_bayesian ... mutate -> log_mutate -> cbind -> cbind -> data.frame In addition: There were 33 warnings (use warnings() to see them) Execution halted
 

I will skip these for now and then try to rerun before troubleshooting. There's really no reason that some runs should work over other's given that they are all reading in the same filetypes 


9/29/25
Command to run the Rscript:
nohup Rscript BayesianAnalysis.R > analysis_run.log 2>&1 &

Added the ngram vs positional runs of the model back in (just commenting out some lines). 


9/12/25 
Had to filter out empty lines because some of the corpora (PhilipsPearl) had an extra, empty line.


8/26/25
We are going to do a clean run of all of the analysis to once and for all ensure that the corpora are standardized, and we don't have any conflicting results.

What we have is five different output corpora for each segmentation strategy corresponding to different random seeds (exception of TP segmentation algorithms). To get the final set of scored list files that will go into the bayesian analysis, we need to do these four steps:

1. Check that the corpora contains both "^" and "x" Klattbet symbols
2. Ensure consistency across the 5 random seeds (delete spaces, convert to set, then compare)
3. Format the files for ngram calculator (each word on its own line and a space between each character)
4. Run each file through ngram calculator using the updated infant stimuli (Canaan sent over email)

TP segmentation needs to be rerun in order to pass it through the pipeline. The TP segmentation strategies are deterministic in data order, so we can't simply do random seeding in the model. Take the corrected PearlBrent input file, delete the spaces in the utterances, produce 5 random shuffles of the data, then run the 6 TP segmentation algorithms on each of the shuffles.


For the incremental implementation, we will choose one of the random seeds, and then extract that file from the AG segmentation files, the JPSD (MaxEnt) segmentation files, and the PUDDLE segmentation files. This will then be run through the incremental code that I have (with the updated infant stimuli sets and removing any standardizing that the code does like replacing x for ^). This code produces spilts in the segmented file (1/512, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2) and removes the segmentation from all other lines (reversing the segmentation of that line's utterance). Thus, we incrementally increase how much of the corpus is segmented under a particular algorithm (up to 1/2 of the corpus), and run each partially segmented corpus through the ngram calculator to track progress on the infant stimuli sets. 
