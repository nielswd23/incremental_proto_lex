nohup: ignoring input
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.1     ✔ stringr   1.5.2
✔ ggplot2   4.0.0     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.1.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

Attaching package: ‘tidylog’

The following objects are masked from ‘package:dplyr’:

    add_count, add_tally, anti_join, count, distinct, distinct_all,
    distinct_at, distinct_if, filter, filter_all, filter_at, filter_if,
    full_join, group_by, group_by_all, group_by_at, group_by_if,
    inner_join, left_join, mutate, mutate_all, mutate_at, mutate_if,
    relocate, rename, rename_all, rename_at, rename_if, rename_with,
    right_join, sample_frac, sample_n, select, select_all, select_at,
    select_if, semi_join, slice, slice_head, slice_max, slice_min,
    slice_sample, slice_tail, summarise, summarise_all, summarise_at,
    summarise_if, summarize, summarize_all, summarize_at, summarize_if,
    tally, top_frac, top_n, transmute, transmute_all, transmute_at,
    transmute_if, ungroup

The following objects are masked from ‘package:tidyr’:

    drop_na, fill, gather, pivot_longer, pivot_wider, replace_na,
    separate_wider_delim, separate_wider_position,
    separate_wider_regex, spread, uncount

The following object is masked from ‘package:stats’:

    filter

Loading required package: Rcpp
Loading 'brms' package (version 2.23.0). Useful instructions
can be found by typing help('brms'). A more detailed introduction
to the package is available through vignette('brms_overview').

Attaching package: ‘brms’

The following object is masked from ‘package:stats’:

    ar


Attaching package: ‘tidybayes’

The following object is masked from ‘package:bayestestR’:

    hdi

The following objects are masked from ‘package:brms’:

    dstudent_t, pstudent_t, qstudent_t, rstudent_t

Loading required package: viridisLite
Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:purrr’:

    lift

group_by: 2 grouping variables (model, fold)
summarise: now 135 rows and 3 columns, ungrouped
[1] "All models have matching unigram, bigram, and both contrast files for each fold."
[1] "working on current model AG_Utt-T-X-X-Seg"
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in unigram_contrast_addins     0
           > rows only in unigram_contrast_stimuli (  0)
           > matched rows                           660    (includes duplicates)
           >                                       =====
           > rows total                             660
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in bigram_contrast_addins     0
           > rows only in bigram_contrast_stimuli (  0)
           > matched rows                          660    (includes duplicates)
           >                                      =====
           > rows total                            660
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in both_contrast_addins     0
           > rows only in both_contrast_stimuli (  0)
           > matched rows                        660    (includes duplicates)
           >                                    =====
           > rows total                          660
[1] "READING in unigram"
# A tibble: 660 × 29
    ...1 SoundOrthography Klattbet Arpabet List  Contrast Source Unigram Bigram
   <int> <chr>            <chr>    <chr>   <chr> <chr>    <chr>    <dbl>  <dbl>
 1     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 2     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 3     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 4     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 5     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 6     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 7     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 8     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 9     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
10     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
# ℹ 650 more rows
# ℹ 20 more variables: KlattbetAdjusted <chr>, KlattbetAdjustedSpaced <chr>,
#   word_len <int>, uni_prob <dbl>, uni_prob_freq_weighted <dbl>,
#   bi_prob <dbl>, bi_prob_freq_weighted <dbl>, bi_prob_smoothed <dbl>,
#   bi_prob_freq_weighted_smoothed <dbl>, pos_uni_score <dbl>,
#   pos_uni_score_freq_weighted <dbl>, pos_uni_score_smoothed <dbl>,
#   pos_uni_score_freq_weighted_smoothed <dbl>, pos_bi_score <dbl>, …
[1] "START FIT_AND_PREDICT fct"
[1] "more than one RS value"
[1] "fitting main model"
Compiling Stan program...
Start sampling

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 9e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.9 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 9e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.9 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 8.4e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.84 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 8.4e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.84 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.747 seconds (Warm-up)
Chain 4:                0.61 seconds (Sampling)
Chain 4:                1.357 seconds (Total)
Chain 4: 
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.78 seconds (Warm-up)
Chain 3:                0.686 seconds (Sampling)
Chain 3:                1.466 seconds (Total)
Chain 3: 
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.735 seconds (Warm-up)
Chain 1:                0.757 seconds (Sampling)
Chain 1:                1.492 seconds (Total)
Chain 1: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.825 seconds (Warm-up)
Chain 2:                0.699 seconds (Sampling)
Chain 2:                1.524 seconds (Total)
Chain 2: 
[1] "kfolding"
Fitting model 1 out of 10
Start sampling
Fitting model 2 out of 10
Start sampling
Fitting model 3 out of 10
Start sampling
Fitting model 4 out of 10
Start sampling
Fitting model 5 out of 10
Start sampling
Fitting model 6 out of 10
Start sampling
Fitting model 7 out of 10
Start sampling
Fitting model 8 out of 10
Start sampling
Fitting model 9 out of 10
Start sampling
Fitting model 10 out of 10
Start sampling
[1] "kfold predicting"
rename: renamed one variable (DrawNum)
pivot_longer: reorganized (1, 2, 3, 4, 5, …) into (ItemClassID, Predicted) [was 4000x661, now 2640000x3]
rename: renamed 2 variables (ItemClassID, GroundTruth)
[1] "PREDS:"
# A tibble: 6 × 3
  DrawNum ItemClassID Predicted
  <chr>   <chr>           <int>
1 1       1                   1
2 1       2                   1
3 1       3                   1
4 1       4                   1
5 1       5                   0
6 1       6                   0
[1] "ACTUALS:"
  ItemClassID GroundTruth
1           1           0
2           2           0
3           3           0
4           4           0
5           5           0
6           6           0
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
[1] "t_a:"
# A tibble: 1 × 6
  accuracy .lower .upper .width .point .interval
     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
1    0.583  0.547  0.617   0.95 median hdi      
[1] "reading in the null model"
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
rename: renamed one variable (OverlapWithRandom)
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
select: dropped one variable (DrawNum)
mutate: new variable 'is_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_random' (double) with one unique value and 0% NA
select: dropped 2 variables (accuracy, is_above_chance)
distinct: removed 3,999 rows (>99%), one row remaining
[1] "t_b"
# A tibble: 1 × 2
  p_above_chance p_above_random
           <dbl>          <dbl>
1              1          0.578
[1] "o"
[1] "Created t_a, t_b, and o. About to run bind_cols"
mutate: new variable 'list' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
[1] "returning results"
[1] "READING in bigram"
# A tibble: 660 × 29
    ...1 SoundOrthography Arpabet Klattbet List  Contrast Source Unigram Bigram
   <int> <chr>            <chr>   <chr>    <chr> <chr>    <chr>    <dbl>  <dbl>
 1     1 boin             B OY N  bOn      High  Bigram   Novel     1.15   1.00
 2     1 boin             B OY N  bOn      High  Bigram   Novel     1.15   1.00
 3     1 boin             B OY N  bOn      High  Bigram   Novel     1.15   1.00
 4     1 boin             B OY N  bOn      High  Bigram   Novel     1.15   1.00
 5     1 boin             B OY N  bOn      High  Bigram   Novel     1.15   1.00
 6     2 boir             B OY R  bOr      Low   Bigram   Novel     1.13   1.00
 7     2 boir             B OY R  bOr      Low   Bigram   Novel     1.13   1.00
 8     2 boir             B OY R  bOr      Low   Bigram   Novel     1.13   1.00
 9     2 boir             B OY R  bOr      Low   Bigram   Novel     1.13   1.00
10     2 boir             B OY R  bOr      Low   Bigram   Novel     1.13   1.00
# ℹ 650 more rows
# ℹ 20 more variables: KlattbetAdjusted <chr>, KlattbetAdjustedSpaced <chr>,
#   word_len <int>, uni_prob <dbl>, uni_prob_freq_weighted <dbl>,
#   bi_prob <dbl>, bi_prob_freq_weighted <dbl>, bi_prob_smoothed <dbl>,
#   bi_prob_freq_weighted_smoothed <dbl>, pos_uni_score <dbl>,
#   pos_uni_score_freq_weighted <dbl>, pos_uni_score_smoothed <dbl>,
#   pos_uni_score_freq_weighted_smoothed <dbl>, pos_bi_score <dbl>, …
[1] "START FIT_AND_PREDICT fct"
[1] "more than one RS value"
[1] "fitting main model"
Compiling Stan program...
Start sampling

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 8.3e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.83 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000177 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.77 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 7.9e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 8e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.436 seconds (Warm-up)
Chain 1:                0.322 seconds (Sampling)
Chain 1:                0.758 seconds (Total)
Chain 1: 
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.4 seconds (Warm-up)
Chain 3:                0.352 seconds (Sampling)
Chain 3:                0.752 seconds (Total)
Chain 3: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.427 seconds (Warm-up)
Chain 2:                0.353 seconds (Sampling)
Chain 2:                0.78 seconds (Total)
Chain 2: 
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.462 seconds (Warm-up)
Chain 4:                0.419 seconds (Sampling)
Chain 4:                0.881 seconds (Total)
Chain 4: 
[1] "kfolding"
Fitting model 1 out of 10
Start sampling
Fitting model 2 out of 10
Start sampling
Fitting model 3 out of 10
Start sampling
Fitting model 4 out of 10
Start sampling
Fitting model 5 out of 10
Start sampling
Fitting model 6 out of 10
Start sampling
Fitting model 7 out of 10
Start sampling
Fitting model 8 out of 10
Start sampling
Fitting model 9 out of 10
Start sampling
Fitting model 10 out of 10
Start sampling
[1] "kfold predicting"
rename: renamed one variable (DrawNum)
pivot_longer: reorganized (1, 2, 3, 4, 5, …) into (ItemClassID, Predicted) [was 4000x661, now 2640000x3]
rename: renamed 2 variables (ItemClassID, GroundTruth)
[1] "PREDS:"
# A tibble: 6 × 3
  DrawNum ItemClassID Predicted
  <chr>   <chr>           <int>
1 1       1                   1
2 1       2                   1
3 1       3                   1
4 1       4                   1
5 1       5                   0
6 1       6                   0
[1] "ACTUALS:"
  ItemClassID GroundTruth
1           1           0
2           2           0
3           3           0
4           4           0
5           5           0
6           6           1
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
[1] "t_a:"
# A tibble: 1 × 6
  accuracy .lower .upper .width .point .interval
     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
1    0.544  0.504  0.578   0.95 median hdi      
[1] "reading in the null model"
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
rename: renamed one variable (OverlapWithRandom)
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
select: dropped one variable (DrawNum)
mutate: new variable 'is_above_chance' (double) with 2 unique values and 0% NA
        new variable 'p_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_random' (double) with one unique value and 0% NA
select: dropped 2 variables (accuracy, is_above_chance)
distinct: removed 3,999 rows (>99%), one row remaining
[1] "t_b"
# A tibble: 1 × 2
  p_above_chance p_above_random
           <dbl>          <dbl>
1          0.986         0.0278
[1] "o"
[1] "Created t_a, t_b, and o. About to run bind_cols"
mutate: new variable 'list' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
[1] "returning results"
[1] "START FIT_AND_PREDICT fct"
[1] "more than one RS value"
[1] "fitting main model"
Compiling Stan program...
Start sampling

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 7.7e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.77 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 8e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 7.6e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 7.3e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.463 seconds (Warm-up)
Chain 1:                0.328 seconds (Sampling)
Chain 1:                0.791 seconds (Total)
Chain 1: 
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.439 seconds (Warm-up)
Chain 3:                0.318 seconds (Sampling)
Chain 3:                0.757 seconds (Total)
Chain 3: 
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.445 seconds (Warm-up)
Chain 4:                0.341 seconds (Sampling)
Chain 4:                0.786 seconds (Total)
Chain 4: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.493 seconds (Warm-up)
Chain 2:                0.39 seconds (Sampling)
Chain 2:                0.883 seconds (Total)
Chain 2: 
[1] "kfolding"
Fitting model 1 out of 10
Start sampling
Fitting model 2 out of 10
Start sampling
Fitting model 3 out of 10
Start sampling
Fitting model 4 out of 10
Start sampling
Fitting model 5 out of 10
Start sampling
Fitting model 6 out of 10
Start sampling
Fitting model 7 out of 10
Start sampling
Fitting model 8 out of 10
Start sampling
Fitting model 9 out of 10
Start sampling
Fitting model 10 out of 10
Start sampling
[1] "kfold predicting"
rename: renamed one variable (DrawNum)
pivot_longer: reorganized (1, 2, 3, 4, 5, …) into (ItemClassID, Predicted) [was 4000x661, now 2640000x3]
rename: renamed 2 variables (ItemClassID, GroundTruth)
[1] "PREDS:"
# A tibble: 6 × 3
  DrawNum ItemClassID Predicted
  <chr>   <chr>           <int>
1 1       1                   0
2 1       2                   0
3 1       3                   0
4 1       4                   0
5 1       5                   0
6 1       6                   0
[1] "ACTUALS:"
  ItemClassID GroundTruth
1           1           0
2           2           0
3           3           0
4           4           0
5           5           0
6           6           0
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
[1] "t_a:"
# A tibble: 1 × 6
  accuracy .lower .upper .width .point .interval
     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
1    0.665  0.636    0.7   0.95 median hdi      
[1] "reading in the null model"
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
rename: renamed one variable (OverlapWithRandom)
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
select: dropped one variable (DrawNum)
mutate: new variable 'is_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_random' (double) with one unique value and 0% NA
select: dropped 2 variables (accuracy, is_above_chance)
distinct: removed 3,999 rows (>99%), one row remaining
[1] "t_b"
# A tibble: 1 × 2
  p_above_chance p_above_random
           <dbl>          <dbl>
1              1              1
[1] "o"
[1] "Created t_a, t_b, and o. About to run bind_cols"
mutate: new variable 'list' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
[1] "returning results"
   accuracy    .lower    .upper .width .point .interval p_above_chance
1 0.5833333 0.5469697 0.6166667   0.95 median       hdi         1.0000
2 0.5439394 0.5041149 0.5783574   0.95 median       hdi         0.9865
3 0.6651515 0.6363636 0.7000000   0.95 median       hdi         1.0000
  p_above_random OverlapWithRandom             list            model
1        0.57750        0.09268744 unigram_contrast AG_Utt-T-X-X-Seg
2        0.02775        0.16413446  bigram_contrast AG_Utt-T-X-X-Seg
3        1.00000        0.00363478    both_contrast AG_Utt-T-X-X-Seg
[1] "working on current model AG_Utt-X-T-X-Seg"
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
rename: renamed one variable (KlattbetAdjustedSpaced)
mutate: new variable 'KlattbetAdjusted' (character) with 132 unique values and 0% NA
        new variable 'RS' (character) with one unique value and 0% NA
        new variable 'model' (character) with one unique value and 0% NA
        new variable 'contrast' (character) with one unique value and 0% NA
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in unigram_contrast_addins     0
           > rows only in unigram_contrast_stimuli (  0)
           > matched rows                           660    (includes duplicates)
           >                                       =====
           > rows total                             660
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in bigram_contrast_addins     0
           > rows only in bigram_contrast_stimuli (  0)
           > matched rows                          660    (includes duplicates)
           >                                      =====
           > rows total                            660
left_join: added 19 columns (KlattbetAdjustedSpaced, word_len, uni_prob, uni_prob_freq_weighted, bi_prob, …)
           > rows only in both_contrast_addins     0
           > rows only in both_contrast_stimuli (  0)
           > matched rows                        660    (includes duplicates)
           >                                    =====
           > rows total                          660
[1] "READING in unigram"
# A tibble: 660 × 29
    ...1 SoundOrthography Klattbet Arpabet List  Contrast Source Unigram Bigram
   <int> <chr>            <chr>    <chr>   <chr> <chr>    <chr>    <dbl>  <dbl>
 1     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 2     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 3     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 4     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 5     1 footh            fuT      F UW TH High  Unigram  JL        1.08   1.00
 6     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 7     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 8     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
 9     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
10     2 geeth            giT      G IY TH High  Unigram  JL        1.07   1.00
# ℹ 650 more rows
# ℹ 20 more variables: KlattbetAdjusted <chr>, KlattbetAdjustedSpaced <chr>,
#   word_len <int>, uni_prob <dbl>, uni_prob_freq_weighted <dbl>,
#   bi_prob <dbl>, bi_prob_freq_weighted <dbl>, bi_prob_smoothed <dbl>,
#   bi_prob_freq_weighted_smoothed <dbl>, pos_uni_score <dbl>,
#   pos_uni_score_freq_weighted <dbl>, pos_uni_score_smoothed <dbl>,
#   pos_uni_score_freq_weighted_smoothed <dbl>, pos_bi_score <dbl>, …
[1] "START FIT_AND_PREDICT fct"
[1] "more than one RS value"
[1] "fitting main model"
Compiling Stan program...
Start sampling

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 7.9e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 8e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 7.4e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 8.1e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.81 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.521 seconds (Warm-up)
Chain 1:                0.43 seconds (Sampling)
Chain 1:                0.951 seconds (Total)
Chain 1: 
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.546 seconds (Warm-up)
Chain 3:                0.434 seconds (Sampling)
Chain 3:                0.98 seconds (Total)
Chain 3: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.568 seconds (Warm-up)
Chain 2:                0.431 seconds (Sampling)
Chain 2:                0.999 seconds (Total)
Chain 2: 
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.549 seconds (Warm-up)
Chain 4:                0.437 seconds (Sampling)
Chain 4:                0.986 seconds (Total)
Chain 4: 
[1] "kfolding"
Fitting model 1 out of 10
Start sampling
Fitting model 2 out of 10
Start sampling
Fitting model 3 out of 10
Start sampling
Fitting model 4 out of 10
Start sampling
Fitting model 5 out of 10
Start sampling
Fitting model 6 out of 10
Start sampling
Fitting model 7 out of 10
Start sampling
Fitting model 8 out of 10
Start sampling
Fitting model 9 out of 10
Start sampling
Fitting model 10 out of 10
Start sampling
[1] "kfold predicting"
rename: renamed one variable (DrawNum)
pivot_longer: reorganized (1, 2, 3, 4, 5, …) into (ItemClassID, Predicted) [was 4000x661, now 2640000x3]
rename: renamed 2 variables (ItemClassID, GroundTruth)
[1] "PREDS:"
# A tibble: 6 × 3
  DrawNum ItemClassID Predicted
  <chr>   <chr>           <int>
1 1       1                   0
2 1       2                   0
3 1       3                   0
4 1       4                   1
5 1       5                   1
6 1       6                   1
[1] "ACTUALS:"
  ItemClassID GroundTruth
1           1           0
2           2           0
3           3           0
4           4           0
5           5           0
6           6           0
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
[1] "t_a:"
# A tibble: 2 × 6
  accuracy .lower .upper .width .point .interval
     <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
1    0.711  0.658  0.661   0.95 median hdi      
2    0.711  0.662  0.759   0.95 median hdi      
[1] "reading in the null model"
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
ungroup: no grouping variables remain
select: dropped one variable (DrawNum)
rename: renamed one variable (OverlapWithRandom)
Joining with `by = join_by(ItemClassID)`
left_join: added one column (GroundTruth)
           > rows only in preds            0
           > rows only in actuals (        0)
           > matched rows          2,640,000
           >                      ===========
           > rows total            2,640,000
mutate: new variable 'Correct' (double) with 2 unique values and 0% NA
group_by: one grouping variable (DrawNum)
summarise: now 4,000 rows and 2 columns, ungrouped
select: dropped one variable (DrawNum)
mutate: new variable 'is_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_chance' (double) with one unique value and 0% NA
        new variable 'p_above_random' (double) with one unique value and 0% NA
select: dropped 2 variables (accuracy, is_above_chance)
distinct: removed 3,999 rows (>99%), one row remaining
[1] "t_b"
# A tibble: 1 × 2
  p_above_chance p_above_random
           <dbl>          <dbl>
1              1              1
[1] "o"
[1] "Created t_a, t_b, and o. About to run bind_cols"
Error in data.frame(..., check.names = FALSE) : 
  arguments imply differing number of rows: 2, 1
Calls: fit_and_predict_bayesian ... mutate -> log_mutate -> cbind -> cbind -> data.frame
In addition: Warning messages:
1: There were 1 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: Examine the pairs() plot to diagnose sampling problems
 
3: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
4: There were 3 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
5: Examine the pairs() plot to diagnose sampling problems
 
6: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
Execution halted
